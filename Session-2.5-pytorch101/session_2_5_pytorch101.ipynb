{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "CWCUuthV4ib3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_train_set = torchvision.datasets.FashionMNIST(\n",
        "    root = './data',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        ")"
      ],
      "metadata": {
        "id": "JQHwdu794tX8"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MnistWithRandomNumberDataset(Dataset):\n",
        "  def __init__(self, mnist_data, random_nums):\n",
        "    self.mnist_data = mnist_data\n",
        "    self.random_nums = random_nums\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.mnist_data)\n",
        "\n",
        "  def __getitem__(self, loc):\n",
        "    img, label = self.mnist_data[loc]\n",
        "    random_num = self.random_nums[loc]\n",
        "    sum = label + random_num\n",
        "    return img, label, random_num, sum"
      ],
      "metadata": {
        "id": "E6oHUqLI7uks"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "random.seed(23)\n",
        "\n",
        "# Load MNIST data and random numbers\n",
        "random_nums = [random.randint(0, 9) for i in range(len(mnist_train_set))]\n",
        "dataset = MnistWithRandomNumberDataset(mnist_train_set, random_nums)\n",
        "train_loader = DataLoader(dataset, batch_size = 64, shuffle = True)"
      ],
      "metadata": {
        "id": "fqdsnSn49tdm"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4FA7R14sgxn",
        "outputId": "40788da3-8f3b-4465-e095-cceb5270a932"
      },
      "source": [
        "import torch.optim as optim\n",
        "torch.set_grad_enabled(True)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7f626c6cca90>"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkAoo1TSsY_n"
      },
      "source": [
        "def get_num_correct(preds, labels):\n",
        "  return preds.argmax(dim=1).eq(labels).sum().item()"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first batch\n",
        "batch = next(iter(train_loader))\n",
        "\n",
        "# Extract the data and label\n",
        "images, true_labels, random_nums, true_sums = batch"
      ],
      "metadata": {
        "id": "leLeOd4O-x5b"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Network, self).__init__()\n",
        "    \n",
        "    # Convolutional layers to process the image\n",
        "    self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "    self.conv3 = nn.Conv2d(64, 128, kernel_size=3)    \n",
        "    \n",
        "    # Fully connected layers to process the image\n",
        "    self.fc1 = nn.Linear(in_features=128, out_features=120)\n",
        "    self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
        "    self.out = nn.Linear(in_features=60, out_features=10)\n",
        "    \n",
        "  def forward(self, x, r):\n",
        "    #first conv layer\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "\n",
        "    #second conv layer\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "\n",
        "    #third conv layer\n",
        "    x = self.conv3(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "\n",
        "    # first fully connected layer\n",
        "    x = x.view(-1, 128)\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "\n",
        "    # second fully connected layer\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.out(x)\n",
        "\n",
        "    #here we are combining the predicted image label and the the random number using one hot encoding\n",
        "    max_indices_x = torch.argmax(x, dim=1)\n",
        "    summed_indices = max_indices_x + r\n",
        "    sum = F.one_hot(summed_indices, num_classes=19) \n",
        "    sum = sum.to(dtype=torch.float32)\n",
        "    sum.requires_grad_()\n",
        "    return x, sum"
      ],
      "metadata": {
        "id": "L5lTrGFZ4-QM"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**building network for single batch**"
      ],
      "metadata": {
        "id": "sLGzBbi3qQnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network = Network()\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size = 64, shuffle = True)\n",
        "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
        "\n",
        "batch = next(iter(train_loader)) # Get Batch\n",
        "images, labels, random_nums, sums = batch\n",
        "\n",
        "label_preds, sum_preds = network(images, random_nums) # Pass Batch\n",
        "label_loss = F.cross_entropy(label_preds, labels)\n",
        "sum_loss = F.cross_entropy(sum_preds, sums)\n",
        "loss = 0.5 * (label_loss + sum_loss) # Calculate Loss\n",
        "print('loss1:', loss.item())\n",
        "print('correct1:', get_num_correct(label_preds, labels))\n",
        "optimizer.zero_grad()\n",
        "label_loss.backward() # Calculate Gradients\n",
        "optimizer.step() # Update Weights\n",
        "\n",
        "\n",
        "label_preds, sum_preds = network(images, random_nums) # Pass Batch\n",
        "label_loss = F.cross_entropy(label_preds, labels)\n",
        "sum_loss = F.cross_entropy(sum_preds, sums)\n",
        "loss = 0.5 * (label_loss + sum_loss) # Calculate Loss\n",
        "print('loss2:', loss.item())\n",
        "print('correct2:', get_num_correct(label_preds, labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PByI7N7ysIzS",
        "outputId": "aae55f56-5c8d-4f64-d426-97495c506259"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss1: 2.598670244216919\n",
            "correct1: 9\n",
            "loss2: 2.5742971897125244\n",
            "correct2: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W3RlJj5o7WTf"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Doing for multiple epochs and batches**"
      ],
      "metadata": {
        "id": "xNsTXf_b72Se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset, batch_size = 64, shuffle = True)\n",
        "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(20):\n",
        "\n",
        "    total_loss = 0\n",
        "    total_correct_label = 0\n",
        "    total_loss_label = 0\n",
        "    total_correct_sum = 0\n",
        "    total_loss_sum = 0\n",
        "\n",
        "    for batch in train_loader: # Get Batch\n",
        "        images, labels, random_nums, sums = batch \n",
        "\n",
        "        label_preds, sum_preds = network(images, random_nums) # Pass Batch\n",
        "        label_loss = F.cross_entropy(label_preds, labels)\n",
        "        sum_loss = F.cross_entropy(sum_preds, sums)\n",
        "        loss = 0.5 * (label_loss + sum_loss) # Calculate Loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward() # Calculate Gradients\n",
        "        optimizer.step() # Update Weights\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_loss_label += label_loss.item()\n",
        "        total_loss_sum += sum_loss.item()\n",
        "        total_correct_label += get_num_correct(label_preds, labels)\n",
        "        total_correct_sum += get_num_correct(sum_preds, sums)\n",
        "\n",
        "    print(\n",
        "        \"epoch\", epoch, \n",
        "        \"total_correct_label:\", total_correct_label, \n",
        "        \"total_loss_label:\", total_loss_label,\n",
        "        \"total_correct_sum:\", total_correct_sum,\n",
        "        \"total_loss_sum:\", total_loss_sum,\n",
        "        \"loss:\", total_loss\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NL18EjXy79ES",
        "outputId": "e7a902e6-66cd-4f33-90f7-f7bfdb1ef45c"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 total_correct_label: 51314 total_loss_label: 379.30297972261906 total_correct_sum: 51314 total_loss_sum: 2040.8281605243683 loss: 1210.0655698776245\n",
            "epoch 1 total_correct_label: 51788 total_loss_label: 353.96588522940874 total_correct_sum: 51788 total_loss_sum: 2033.4219107627869 loss: 1193.6938980817795\n",
            "epoch 2 total_correct_label: 52151 total_loss_label: 344.57045044004917 total_correct_sum: 52151 total_loss_sum: 2027.7656593322754 loss: 1186.1680557727814\n",
            "epoch 3 total_correct_label: 52180 total_loss_label: 337.7876736074686 total_correct_sum: 52180 total_loss_sum: 2027.359409570694 loss: 1182.5735409259796\n",
            "epoch 4 total_correct_label: 52216 total_loss_label: 339.0338530316949 total_correct_sum: 52216 total_loss_sum: 2026.7969098091125 loss: 1182.9153792858124\n",
            "epoch 5 total_correct_label: 52552 total_loss_label: 324.54372161626816 total_correct_sum: 52552 total_loss_sum: 2021.5156593322754 loss: 1173.0296909809113\n",
            "epoch 6 total_correct_label: 52591 total_loss_label: 326.89600083976984 total_correct_sum: 52591 total_loss_sum: 2021.0000357627869 loss: 1173.9480185508728\n",
            "epoch 7 total_correct_label: 52393 total_loss_label: 335.1514900177717 total_correct_sum: 52393 total_loss_sum: 2024.000033378601 loss: 1179.5757628679276\n",
            "epoch 8 total_correct_label: 52737 total_loss_label: 322.6487879753113 total_correct_sum: 52737 total_loss_sum: 2018.6875343322754 loss: 1170.6681623458862\n",
            "epoch 9 total_correct_label: 52687 total_loss_label: 324.9943139180541 total_correct_sum: 52687 total_loss_sum: 2019.4375350475311 loss: 1172.2159250974655\n",
            "epoch 10 total_correct_label: 53033 total_loss_label: 305.8706995919347 total_correct_sum: 53033 total_loss_sum: 2014.046909570694 loss: 1159.9588059186935\n",
            "epoch 11 total_correct_label: 52697 total_loss_label: 323.9645846039057 total_correct_sum: 52697 total_loss_sum: 2019.2656593322754 loss: 1171.6151212453842\n",
            "epoch 12 total_correct_label: 52922 total_loss_label: 313.1049950271845 total_correct_sum: 52922 total_loss_sum: 2015.7656588554382 loss: 1164.4353275299072\n",
            "epoch 13 total_correct_label: 52697 total_loss_label: 331.4623031914234 total_correct_sum: 52697 total_loss_sum: 2019.2656598091125 loss: 1175.3639818429947\n",
            "epoch 14 total_correct_label: 52957 total_loss_label: 314.1307446360588 total_correct_sum: 52957 total_loss_sum: 2015.1406593322754 loss: 1164.6357012987137\n",
            "epoch 15 total_correct_label: 52426 total_loss_label: 347.6105978861451 total_correct_sum: 52426 total_loss_sum: 2023.5000350475311 loss: 1185.5553150177002\n",
            "epoch 16 total_correct_label: 52954 total_loss_label: 322.413120046258 total_correct_sum: 52954 total_loss_sum: 2015.2344088554382 loss: 1168.8237637281418\n",
            "epoch 17 total_correct_label: 53091 total_loss_label: 313.45527363568544 total_correct_sum: 53091 total_loss_sum: 2013.1250331401825 loss: 1163.2901521921158\n",
            "epoch 18 total_correct_label: 53361 total_loss_label: 295.9490918368101 total_correct_sum: 53361 total_loss_sum: 2008.859409570694 loss: 1152.4042514562607\n",
            "epoch 19 total_correct_label: 53092 total_loss_label: 313.57266400009394 total_correct_sum: 53092 total_loss_sum: 2013.1406586170197 loss: 1163.3566620349884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WgUAexSF8VSV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}