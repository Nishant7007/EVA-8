{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "CWCUuthV4ib3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_train_set = torchvision.datasets.FashionMNIST(\n",
        "    root = './data',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        ")"
      ],
      "metadata": {
        "id": "JQHwdu794tX8"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MnistWithRandomNumberDataset(Dataset):\n",
        "  def __init__(self, mnist_data, random_nums):\n",
        "    self.mnist_data = mnist_data\n",
        "    self.random_nums = random_nums\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.mnist_data)\n",
        "\n",
        "  def __getitem__(self, loc):\n",
        "    img, label = self.mnist_data[loc]\n",
        "    random_num = self.random_nums[loc]\n",
        "    sum = label + random_num\n",
        "    # print(label, random_num, sum)\n",
        "    # random_num_one_hot = F.one_hot(torch.tensor(random_num), num_classes=10)\n",
        "    # Convert the tensor to dtype torch.float32\n",
        "    # random_num_one_hot = random_num_one_hot.to(dtype=torch.float64)\n",
        "    # print(random_num_one_hot)\n",
        "    # sum_one_hot = F.one_hot(torch.tensor(sum), num_classes=19)\n",
        "    return img, label, random_num, sum"
      ],
      "metadata": {
        "id": "E6oHUqLI7uks"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "random.seed(23)\n",
        "\n",
        "# Load MNIST data and random numbers\n",
        "random_nums = [random.randint(0, 9) for i in range(len(mnist_train_set))]\n",
        "dataset = MnistWithRandomNumberDataset(mnist_train_set, random_nums)\n",
        "train_loader = DataLoader(dataset, batch_size = 64, shuffle = True)"
      ],
      "metadata": {
        "id": "fqdsnSn49tdm"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4FA7R14sgxn",
        "outputId": "40788da3-8f3b-4465-e095-cceb5270a932"
      },
      "source": [
        "import torch.optim as optim\n",
        "torch.set_grad_enabled(True)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7f626c6cca90>"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkAoo1TSsY_n"
      },
      "source": [
        "def get_num_correct(preds, labels):\n",
        "  return preds.argmax(dim=1).eq(labels).sum().item()"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first batch\n",
        "batch = next(iter(train_loader))\n",
        "\n",
        "# Extract the data and label\n",
        "images, true_labels, random_nums, true_sums = batch"
      ],
      "metadata": {
        "id": "leLeOd4O-x5b"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Network, self).__init__()\n",
        "    \n",
        "    # Convolutional layers to process the image\n",
        "    self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "    self.conv3 = nn.Conv2d(64, 128, kernel_size=3)\n",
        "    self.conv4 = nn.Conv2d(128, 256, kernel_size=3)\n",
        "    \n",
        "    \n",
        "    # Fully connected layers to process the image\n",
        "    self.fc1 = nn.Linear(in_features=128, out_features=120)\n",
        "    self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
        "    self.out = nn.Linear(in_features=60, out_features=10)\n",
        "    \n",
        "  def forward(self, x, r):\n",
        "    # print(\"x shape1: \", x.shape)\n",
        "    x = self.conv1(x)\n",
        "    # print(\"x shape2: \", x.shape)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "\n",
        "    # print(\"x shape3: \", x.shape)\n",
        "    x = self.conv2(x)\n",
        "    # print(\"x shape4: \", x.shape)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "\n",
        "\n",
        "    x = self.conv3(x)\n",
        "    # print(\"x shape5: \", x.shape)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "    # print(\"x shape6: \", x.shape)\n",
        "\n",
        "    x = x.view(-1, 128)\n",
        "    # print(\"x shape7: \", x.shape)\n",
        "    x = self.fc1(x)\n",
        "    # print(\"x shape8: \", x.shape)\n",
        "    x = F.relu(x)\n",
        "    # print(\"x shape9: \", x.shape)\n",
        "    x = self.fc2(x)\n",
        "    # print(\"x shape10: \", x.shape)\n",
        "    x = F.relu(x)\n",
        "    # print(\"x shape11: \", x.shape)\n",
        "    x = self.out(x)\n",
        "    # print(\"x shape12: \", x.shape)\n",
        "    # print(\"r shape: \", r.unsqueeze(1).shape)\n",
        "\n",
        "    max_indices_x = torch.argmax(x, dim=1)  # shape: (64,)\n",
        "    summed_indices = max_indices_x + r  # shape: (64,)\n",
        "    sum = F.one_hot(summed_indices, num_classes=19) \n",
        "    sum = sum.to(dtype=torch.float32)\n",
        "    sum.requires_grad_()\n",
        "    # print(\"one_hot_tensor shape: \", sum.shape)\n",
        "    # print(sum)\n",
        "    return x, sum"
      ],
      "metadata": {
        "id": "L5lTrGFZ4-QM"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**building network for single batch**"
      ],
      "metadata": {
        "id": "sLGzBbi3qQnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network = Network()\n",
        "\n",
        "# train_loader = MnistWithRandomNumberDataset(mnist_train_set, random_nums)\n",
        "train_loader = DataLoader(dataset, batch_size = 64, shuffle = True)\n",
        "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
        "\n",
        "batch = next(iter(train_loader)) # Get Batch\n",
        "images, labels, random_nums, sums = batch\n",
        "\n",
        "label_preds, sum_preds = network(images, random_nums) # Pass Batch\n",
        "label_loss = F.cross_entropy(label_preds, labels)\n",
        "sum_loss = F.cross_entropy(sum_preds, sums)\n",
        "loss = 0.5 * (label_loss + sum_loss) # Calculate Loss\n",
        "print('loss1:', loss.item())\n",
        "print('correct1:', get_num_correct(label_preds, labels))\n",
        "optimizer.zero_grad()\n",
        "label_loss.backward() # Calculate Gradients\n",
        "optimizer.step() # Update Weights\n",
        "\n",
        "\n",
        "label_preds, sum_preds = network(images, random_nums) # Pass Batch\n",
        "label_loss = F.cross_entropy(label_preds, labels)\n",
        "sum_loss = F.cross_entropy(sum_preds, sums)\n",
        "loss = 0.5 * (label_loss + sum_loss) # Calculate Loss\n",
        "print('loss2:', loss.item())\n",
        "print('correct2:', get_num_correct(label_preds, labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PByI7N7ysIzS",
        "outputId": "aae55f56-5c8d-4f64-d426-97495c506259"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss1: 2.598670244216919\n",
            "correct1: 9\n",
            "loss2: 2.5742971897125244\n",
            "correct2: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W3RlJj5o7WTf"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Doing for multiple epochs and batches**"
      ],
      "metadata": {
        "id": "xNsTXf_b72Se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset, batch_size = 64, shuffle = True)\n",
        "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(3):\n",
        "\n",
        "    total_loss = 0\n",
        "    total_correct_label = 0\n",
        "    total_loss_label = 0\n",
        "    total_correct_sum = 0\n",
        "    total_loss_sum = 0\n",
        "\n",
        "    for batch in train_loader: # Get Batch\n",
        "        images, labels, random_nums, sums = batch \n",
        "\n",
        "        label_preds, sum_preds = network(images, random_nums) # Pass Batch\n",
        "        label_loss = F.cross_entropy(label_preds, labels)\n",
        "        sum_loss = F.cross_entropy(sum_preds, sums)\n",
        "        loss = 0.5 * (label_loss + sum_loss) # Calculate Loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward() # Calculate Gradients\n",
        "        optimizer.step() # Update Weights\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_loss_label += label_loss.item()\n",
        "        total_loss_sum += sum_loss.item()\n",
        "        total_correct_label += get_num_correct(label_preds, labels)\n",
        "        total_correct_sum += get_num_correct(sum_preds, sums)\n",
        "\n",
        "    print(\n",
        "        \"epoch\", epoch, \n",
        "        \"total_correct_label:\", total_correct_label, \n",
        "        \"total_loss_label:\", total_loss_label,\n",
        "        \"total_correct_sum:\", total_correct_sum,\n",
        "        \"total_loss_sum:\", total_loss_sum,\n",
        "        \"loss:\", total_loss\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NL18EjXy79ES",
        "outputId": "7d173d15-6c3e-4d8f-cdfa-62d0e9dd6e77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 total_correct_label: 45865 total_loss_label: 584.5705933868885 total_correct_sum: 45865 total_loss_sum: 2126.015678882599 loss: 1355.293135046959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WgUAexSF8VSV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}